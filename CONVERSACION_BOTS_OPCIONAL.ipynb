{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d563a-5fec-416a-864a-89c860fff85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos una conversación entre GPT-4o-mini,Claude-3-haiku y Gemini 1.5 flash\n",
    "\n",
    "# Estamos usando versiones económicas de los modelos, por lo que los costos serán mínimos\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "gemini_model = \"gemini-1.5-flash\"\n",
    "\n",
    "# Definición de personalidad para cada chatbot\n",
    "gpt_system = \"Eres un chatbot muy argumentativo; \\\n",
    "no estás de acuerdo con nada en la conversación y cuestionas todo de manera sarcástica.\"\n",
    "\n",
    "claude_system = \"Eres un chatbot muy educado y cortés. Intentas estar de acuerdo con \\\n",
    "todo lo que dice la otra persona o encontrar puntos en común. Si la otra persona discute, \\\n",
    "intentas calmarla y seguir charlando.\"\n",
    "\n",
    "gemini_system = \"Eres un chatbot humorista;\\\n",
    "intentas poner un toque de humor a todo lo que dicen otras personas;\\\n",
    "si la otra persona discute, le pones un toque de ironía y sigues charlando.\"\n",
    "\n",
    "# Inicializamos las listas de mensajes de cada chatbot\n",
    "gpt_messages = [\"¡Hola!\"]\n",
    "claude_messages = [\"Hola\"]\n",
    "gemini_messages = [\"Helloooo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ce9bde-4557-47b5-9bfa-56b50be9e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para hacer que GPT-4o-mini responda\n",
    "def call_gpt():\n",
    "        messages = [{\"role\": \"system\", \"content\": gpt_system}]  # Establecemos la personalidad de GPT\n",
    "     # Recorremos los mensajes previos para darle contexto a GPT\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages,gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt}) # Mensajes de GPT\n",
    "        messages.append({\"role\": \"user\", \"content\":f\"{claude}\\n{gemini}\"}) # Mensajes de Claude y Gemini juntos\n",
    "    # Llamamos a la API de OpenAI para obtener la respuesta de GPT\n",
    "    completion = openai.chatCompletion.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages)\n",
    "    # Devolvemos la respuesta de GPT\n",
    "    return completion.[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78c6dd-5b62-4ce6-9ec0-b4a5a5999471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para hacer que Claude-3-Haiku responda\n",
    "def call_claude():\n",
    "    messages = [] # Lista para almacenar el historial de mensajes\n",
    "    # Recorremos los mensajes previos para darle contexto a Claude\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages,gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"{gpt}\\n{gemini}\"})  # Mensajes de GPT y Gemini juntos\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude})  # Respuesta anterior de Claude\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"{gpt_messages[-1]}\\n{gemini_messages[-1]}\"}) # Últimos mensajes\n",
    "    # Conectamos con la API de Anthropic (Claude)\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500)\n",
    "    # Devolvemos la respuesta de Claude\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f949617-9f5c-4b2e-9ba4-c85530ce8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para hacer que Gemini 1.5 Flash responda\n",
    "def call_gemini():\n",
    "    messages = [{\"role\": \"user\", \"content\": gemini_system}] # Establecemos la personalidad de Gemini\n",
    "    # Recorremos los mensajes previos para darle contexto a Gemini\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages,gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"{gpt}\\n{claude}\"}) # Mensajes de GPT y Claude juntos\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini}) # Mensajes de GPT y Claude juntos\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"{gpt_messages[-1]}\\n {claude_messages[-1]}\"}) # Últimos mensajes\n",
    "    # Conectamos con la API de Google (Gemini)\n",
    "    message = genai.chat(\n",
    "        model=gemini_model,\n",
    "        messages = [{\"role\":\"user\",\"content\":msg[\"content\"]} for msg in messages])\n",
    "    return.message.last # Devolvemos la última respuesta de Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb097b-10f8-4b56-a2d3-5a8a39db1138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CICLO DE CONVERSACIÓN ENTRE LOS BOTS \n",
    "messages = [{\"role\": \"user\", \"content\": \"¡Hola!\"}]  # Mensaje inicial de la conversación\n",
    "\n",
    "for i in range(5):  # Definimos el número de rondas de conversación\n",
    "    print(f\"\\n Ronda {i+1}\")\n",
    "\n",
    "    # GPT responde\n",
    "    gpt_response = call_gpt()\n",
    "    print(f\"GPT: {gpt_response}\")\n",
    "    messages.append({\"role\": \"assistant\", \"content\": gpt_response})\n",
    "    gpt_messages.append(gpt_response)  # Guardamos la respuesta para contexto futuro\n",
    "\n",
    "    # Claude responde\n",
    "    claude_response = call_claude()\n",
    "    print(f\"Claude: {claude_response}\")\n",
    "    messages.append({\"role\": \"assistant\", \"content\": claude_response})\n",
    "    claude_messages.append(claude_response)  # Guardamos la respuesta para contexto futuro\n",
    "\n",
    "    # Gemini responde\n",
    "    gemini_response = call_gemini()\n",
    "    print(f\"Gemini: {gemini_response}\")\n",
    "    messages.append({\"role\": \"assistant\", \"content\": gemini_response})\n",
    "    gemini_messages.append(gemini_response)  # Guardamos la respuesta para contexto futuro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
